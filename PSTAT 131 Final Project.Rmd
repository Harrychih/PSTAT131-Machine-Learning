---
title: "PSTAT 131 Final Project Report"
author: 
- Yanjie Qi
- Wuji Shan
- Zhaochi Ye
geometry: margin=3cm
date: "`r format(Sys.Date(), '%B %d, %Y')`"
header-includes:
- \usepackage{titling}\usepackage{float}
- \pretitle{\begin{center}\LARGE\includegraphics[width=6cm]{UCSBseal.jpg}\\[\bigskipamount]}
- \posttitle{\end{center}}
abstract: |
  Legal of Legends is highly competitive, fast paced action-strategy game designed for those who crave a hard fought victory. This dataset is from the Riot Gaming in the Diamond Rank Game of League of Legend. We are trying to design a model to predict how various factors affect the consequence of a game in the Diamond Rank Game of NA server. After viewing the dataset, we think methods K nearest neighbor, logistic regression, bagging, randomForest and regulation are potential methods for discovering the relationship.
output: 
  pdf_document:
    fig_caption: TRUE
    latex_engine: xelatex
documentclass: report
---

  
```{r setup, echo=FALSE}
library(knitr)
options(digits = 4)  #limit the number of 
```


# Data Overview

## Research Question and :
  - How does various factors in first ten minutes affect the consequence of a game in the Diamond Rank Game of NA server in League of Legends?

  - We hope to develop/use supervised machine learning.

## Data Exploration:
  - Response variable: 
    - ‘blueWins’: the result whether blue side wins
  - Total predictor variables: 38
    Numeric predictor variables: 30
    Binary  predictor variables: 8
  - Predictor varibles:
    - blueWardsPlaced: Number of warding totems placed by the blue team                         on the map
    - blueWardsDestroyed: Number of enemy warding totems the blue team                             has destroyed
    - blueFirstBlood: First kill of the game. 1 if the blue team did the                       first kill, 0 otherwise
    - blueKills: Number of enemies killed by the blue team
    - blueDeaths: Number of deaths (blue team)
    - blueAssists: Number of kill assists (blue team)
    - blueEliteMonster: Number of elite monsters killed by the blue team                         (Dragons and Heralds)
    - blueDragons: Number of dragons killed by the blue team
    - blueHeralds: Number of heralds killed by the blue team
    - blueTowersDestroyed: Number of structures destroyed by the blue                               team (towers...)
    - blueTotalGold: Blue team total gold
    - blueAvgLevel: blue team average champion level
    - blueTotalExperience: Blue team total experience
    - blueTotalMinionsKilled: blue team total minions killed
    - blueTotalJungleMinionsKilled: Blue team total jungle monsters                                          killed
    - blueGoldDiff: blue team gold difference compared to the enemy team
    - blueExperienceDiff: Blue team experience difference compared to the                           enemy team
    - blueCSPerMin: blue team CS (minions) per minute
    - blueGoldPerMin: blue team gold per minute
    - redWardsPlaced: Number of warding totems placed by the red team on                       the map
    - redWardsDestroyed: Number of enemy warding totems the red team                              has destroyed
    - redFirstBlood: First kill of the game. 1 if the red team did the                        first kill, 0 otherwise
    - redKills: Number of enemies killed by the red team
    - redDeaths: Number of deaths (red team)
    - redAssists: Number of kill assists (red team)
    - redEliteMonster: Number of elite monsters killed by the red team                          (Dragons and Heralds)
    - redDragons: Number of dragons killed by the red team
    - redHeralds: Number of heralds killed by the red team
    - redTowersDestroyed: Number of structures destroyed by the red                                team (towers...)
    - redTotalGold: red team total gold
    - redAvgLevel: red team average champion level
    - redTotalExperience: red team total experience
    - redTotalMinionsKilled: red team total minions killed
    - redTotalJungleMinionsKilled: red team total jungle monsters                                           killed
    - redGoldDiff: red team gold difference compared to the enemy team
    - redExperienceDiff: red team experience difference compared to the                           enemy team
    - redCSPerMin: red team CS (minions) per minute
    - redGoldPerMin: red team gold per minute
    
  
## Analysis plan:
  - Descriptive statistics: 
    - We plan to report mean and median on numeric predictor variables, and report frequencies on binary predictor variables.
    - We plan to report boxplots of the blue side and red side in the dataset.
    - We are planning to use supervised machine learning.
  - Model building: 
    - We plan to train our classifier via cross-validation.
  - Model testing:
    - 2964 hold-out observations will be planned.

## Reference:
  - Michel's fanboi. (2020, April). League of Legends Diamond Ranked Games (10 min), Version 1. 
    Retrieved May 27, 2020 from https://www.kaggle.com/bobbyscience/league-of-legends-diamond-ranked-games-10-min


```{r, include=FALSE}
library(tidyverse)
library(ForImp)
library(dplyr)
library(randomForest)
library(gbm)
library(ISLR)
library(tree)
library(class)
```

## Data Pre-processing

```{r data_loading, include=FALSE}
# Load the data
RankDt <- read.csv("~/Box/Final Project/Dataset/high_diamond_ranked_10min.csv")

# Dimension of the data
dim(RankDt)
```

```{r data_cleaning, include=FALSE}
# Primary Data cleaning
# Exclude GameId column since it is obviously not related to the model
RankD <- RankDt %>%
  select(-gameId)
```

```{r, eval=FALSE, include=FALSE}
# Summarize the missingness in the data
for (i in names(RankD)){
  print(which(is.na(RankD$i)))
}
## There is no missing value in the data set     
```
After loading the target dataset, we have the dataset with 40 columns and 9879 observations; among these 40 columns, there are 1 response variable, 38 potential predictor variables, and one 'gameID' variable that is absolutely unrelated to our research. Thus, we eliminated the 'gameID' column and summarised the missness in the data; it turns out that there is no missing value in this dataset.

# Methods
- To split into training and test dataset, we took 70% of random sample in the dataset as our training set and the other 30% as test set. In the traing set, there are 6915 observations; in the test set, there are 2964 observations. 

- To generally understand the response variable, we calculated the frequency of "blueWin"(blueWins == 1) and "blueLose" (blueWins == 0). Among the 6915 training observations, in 3447 games blue side won the game and in 3468 games red side won the game.

```{r data split, include=FALSE}
# set the seed
set.seed(2020)
# training and test dataset
train <- sample(1:nrow(RankD), 0.7*nrow(RankD))
Rank.train <- RankD[train,]
Rank.test <- RankD[-train,]
# dimensions of training set
dim(Rank.train)
# dimensions of test set
dim(Rank.test)
```

```{r winsFrequency, include=FALSE}
# since our response varibale is a binary variable, we intend to have a table to show the frequency of
# wins and loses
# frequency of blue wins
table(Rank.train$blueWins )
```

- To visualize the dataset, we summarized every column to show the overall data distribution:

```{r summaryOfDataset, echo=FALSE}
# Summary of Dataset
summary(RankD)
```

- To better understand the dataset, we employed the boxplot to show the data distribution of total gold of blue under two situations: "blueWins" and "blueLose", since we reckoned the totalGold of blue might be the most related paramater.

```{r boxplot, echo=FALSE, fig.align='center', fig.show='hold'}
# Use qplot to make a boxplot of
blueTotalGold_Wins <- Rank.train %>% 
  select(blueTotalGold, blueWins) %>%
  filter(blueWins == 1)
blueTotalGold_lose <- Rank.train %>% 
  select(blueTotalGold, blueWins) %>%
  filter(blueWins == 0)

par(mfrow = c(2,2))
qplot(x = blueTotalGold, y = blueWins, data = blueTotalGold_Wins, geom = "boxplot")
qplot(x = blueTotalGold, y = blueWins, data = blueTotalGold_lose, geom = "boxplot")
```

## 1. Logistic Regression Method

### Build and summarise a logistic regression model
```{r}
# use logistic regression to explore the relationship
# between bluewins and other predictor variables
# fit the response variable to the selected variable
glm.fit <- glm(blueWins ~ ., 
               data = Rank.train, family = binomial)

# summarize the logistic regression model
summary(glm.fit)
```

### Interpret coefficient

In above result, blueDragons, blueGoldDiff, blueExperienceDiff, redTowersDestroyed, 
redTotalMinionsKilled, and redTotalJungleMinionsKilled are statistically significant at level 0.05.


### Construct confusion matrix for the training data

```{r, include=FALSE}
# Specify type="response" to get the estimated probabilities
prob.training <- predict(glm.fit, type = "response")
# Round the result to 2 decimal places
round(prob.training, digits = 2)
```

```{r}
# Save the predicted labels using 0.5 as a threshold
Rank.train <- Rank.train %>%
  mutate(preBlueWins=as.factor(ifelse(prob.training<=0.5, 0, 1)))
# Confusion matrix (training error/accuracy)
table(pred=Rank.train$preBlueWins, true=Rank.train$blueWins)
```

```{r}
# Accurate rate
Lg_AR <- (2560+2535) / (2560+912+908+2535)
Lg_AR # 0.7368
# Out of All lost game, correct classified rate
LCCR <- 2560 / (2560 + 908)
LCCR # 0.7382
# Out of All winned game, correct classified rate
WCCR <- 2535 / (2535 + 912)
WCCR
```

### Estimate win rate for the test data
```{r}
# Predict the win rate and round the predict results to 5 decimals
prob.test <- round(predict(glm.fit, Rank.test, type = "response"), digits = 5)
Rank.test <- Rank.test %>%
  mutate(winRate=prob.test)
# Predict bluw win or not column
Rank.test <- Rank.test %>%
  mutate(preBlueWins=as.factor(ifelse(prob.test<=0.5, 0, 1)))
```

```{r}
# confusion matrix (test error)
table(pred=Rank.test$preBlueWins, true=Rank.test$blueWins)
```

```{r}
# test accurate rate
Lg_testMR <- (1087+1058) / (1087+425+394+1058)
Lg_testMR
```
The accurate rate is 0.7237, the error rate is 0.2763.

## 2. K-Nearest Neighbor Method

```{r}
# Reload the training and test dataset in case there was change of specific variables
# set the seed
set.seed(2020)
# training and test dataset
train <- sample(1:nrow(RankD), 0.7*nrow(RankD))
Rank.train <- RankD[train,]
Rank.test <- RankD[-train,]
```

```{r}
# YTrain is the observed labels for bluwWins on the training set, XTrain is the design matrix
YTrain = Rank.train$blueWins
XTrain = Rank.train %>% select(-blueWins)
XTrain <- scale(XTrain,center = TRUE, scale = TRUE)
str(XTrain)
```

```{r}
meanvec <- attr(XTrain,'scaled:center')
sdvec <- attr(XTrain,'scaled:scale')
# YTest is the observed labels for blueWins on the test set, Xtest is the design matrix
YTest = Rank.test$blueWins
XTest = Rank.test %>% select(-blueWins) %>% scale(center = meanvec, scale = sdvec)
```

```{r}
# Set validation.error (a vector) to save validation errors in future
validation.error = NULL
# Give possible number of nearest neighbours to be considered
allK = 1:50
# Set random seed to make the results reproducible
set.seed(66)
# For each number in allK, use LOOCV to find a validation error
for (i in allK){ # Loop through different number of neighbors
pred.Yval = knn.cv(train=XTrain, cl=YTrain, k=i) # Predict on the left-out validation set
validation.error = c(validation.error, mean(pred.Yval!=YTrain)) # Combine all validation errors
}
# Validation error for 1-NN, 2-NN, ..., 50-NN
validation.error
```

```{r}
# Best number of neighbors
# if there is a tie, pick larger number of neighbors for simpler model
numneighbor = max(allK[validation.error == min(validation.error)])
numneighbor
```
So we determine the best number for kNN is k = 42.

```{r}
# Set random seed to make the results reproducible
set.seed(67)
# Best k used
pred.YTest = knn(train=XTrain, test=XTest, cl=YTrain, k=numneighbor)
# Confusion matrix
conf.matrix = table(predicted=pred.YTest, true=YTest)
conf.matrix
```

```{r}
# Test accuracy rate
sum(diag(conf.matrix)/sum(conf.matrix))
# Test error rate
1 - sum(diag(conf.matrix)/sum(conf.matrix))
```
The accuracy rate is 0.7078, the test error rate is 0.2922.


## 3. Random Forest & bagging

### Random Forest

```{r}
# import essential packages
library(randomForest)
library(caret)
# use tune module to optimal mtry parameter
set.seed(2020)
fit.rf<-tuneRF(x=Rank.train[,-1:-2],y=as.factor(Rank.train$blueWins),trace=F,doBest=T,plot=T)
# best mtry parameter is selected while OBB error is least

# plot the error trend as ntree parameter increasing
plot(fit.rf)
# according to the result error decreasing and keep stable as ntree increasing
# thus ntree parameter is suitable

# plot the importance of variable
varImpPlot(fit.rf,main='feature important',cex=0.75)


# Cross validation to estimate testing error
# divide the data into 10-fold
CVgroup <- function(k,datasize,seed){
  cvlist <- list()
  set.seed(seed)
  n <- rep(1:k,ceiling(datasize/k))[1:datasize]    
  temp <- sample(n,datasize)   
  x <- 1:k
  dataseq <- 1:datasize
  cvlist <- lapply(x,function(x) dataseq[temp==x])  
  return(cvlist)
}
# use 1 fold as test and others as train
cvtest <- function(i){
  train <- Rank.train[i,]
  test <- Rank.train[-i,]
  fit0 <- randomForest(x=train[,-1:-2],y=as.factor(train$blueWins),mtry=fit.rf$mtry)
  pred<-predict(fit0,test)
  return(1-sum(diag(table(pred,test$blueWins)))/nrow(test))
}
# considered each fold as test one times 
cvlist<-CVgroup(k=10,data=nrow(Rank.train),seed=2020)
cv.res<-lapply(cvlist,cvtest)
# calculate the testing error for 10-flod cross validation
average.error<-mean(unlist(cv.res))
average.error
```

```{r}
# predict
pred<-predict(fit.rf,Rank.test)
# construct the confusion matrix and calculate the accuracy, specifity and sensitivity
confusionMatrix(pred,as.factor(Rank.test$blueWins))
# The accuracy is 0.723, the error rate is 0.277.
```
The accuracy is 0.723, the error rate is 0.277.


### Bagging

```{r}
# Reload the training and test dataset in case there was change of specific variables
# set the seed
set.seed(2020)
# training and test dataset
train <- sample(1:nrow(RankD), 0.7*nrow(RankD))
Rank.train <- RankD[train,]
Rank.test <- RankD[-train,]
```

```{r, include=FALSE, eval=FALSE}
glimpse(Rank.train)
```

```{r}
Rank.train$blueWins <- as.character(Rank.train$blueWins)
Rank.train$blueWins <- as.factor(Rank.train$blueWins)
bag.rank <- randomForest(blueWins~., data = Rank.train, mtry=10, ntree=500, importance = TRUE)
bag.rank
```
```{r}
plot(bag.rank)
legend("top", colnames(bag.rank$err.rate), col = 1:4, cex = 0.8, fill = 1:4)
```

```{r}
# show how the model is well performed in test set
yhat.bag <- predict(bag.rank, newdata = Rank.test)

# Confusion matrix
bag.err <- table(pred=yhat.bag, truth=Rank.test$blueWins)
test.bag.err <- 1- sum(diag(bag.err))/sum(bag.err)
test.bag.err
```
The error rate is 0.281.


## 4. Regulation

```{r}
# import essential packages
library(glmnet)
set.seed(2020)
# perform 10-fold cross validation to find the best lambda parameter
cv.lasso<-cv.glmnet(x=as.matrix(Rank.train[,-1:-2]),y=as.factor(Rank.train$blueWins),family='binomial')
# plot the result of cross validation
plot(cv.lasso)
# according to the result, lambda is selected as testing erroe being least

# fit the lasso regression model as the best lambda where testing error is least
fit.lasso<-glmnet(x=as.matrix(Rank.train[,-1:-2]),y=as.factor(Rank.train$blueWins),family='binomial',lambda=cv.lasso$lambda.min)

# plot feature importance evaluated by coefficient 
barplot(as.numeric(fit.lasso$beta),horiz = T,names.arg = rownames(fit.lasso$beta),las=1,cex.names=0.5,main='Feature importance (coefficients)')

# predict
pred<-predict(fit.lasso,as.matrix(Rank.test[,-1:-2]))
pred<-as.factor(ifelse(pred>0,1,0))

# construct the confusion matrix and calculate the accuracy, specifity and sensitivity
confusionMatrix(pred,as.factor(Rank.test$blueWins))
# The accuracy rate is 0.722, the error rate is 0.278.
```
The accuracy rate is 0.722, the error rate is 0.278.



## Data Analysis

From the models above, the error rate of logistic regression model is 0.2763, error rate of k-NN model is 0.2922, error rate of randomForest is 0.277, error rate of bagging is 0.281, error rate of regulation is 0.278.
We can find that 0.2763 is the smallest error rate, so we can conclude that for this dataset, logistic regression model is the best model.

In logistic regression model summary results, blueDragons, blueGoldDiff, blueExperienceDiff, redTowersDestroyed, redTotalMinionsKilled, and redTotalJungleMinionsKilled are statistically significant at level 0.05.

